Discussion here, in present/past tense.

\subsection{Interpretation of LWTA Networks}
    From \cref{res:fig:LWTA_architecture_trained01} and \cref{res:fig:LWTA_architecture_trained45} it is clear that separate pathways are formed that are specialised on input that are to be categorised differently. The untrained network \cref{res:fig:LWTA_architecture_untrained} displays no such patterns. In \cref{res:fig:LWTA_architecture_trained01} we can see that the lowest node in the middle layer strongly correlates with results that are classified as 0, with clear pathways being formed with the same nodes in the groups of the layers before and after. This coincides with our expectations based on the neurobiological pathways and feature selectivity. A further analysis of different inputs could help give insight into what these pathways have learned, be it roundness of the digits, amount of dark vs. light, digit placement in the picture, or perhaps some completely different and unintuitive feature. This provides a way to start extracting information about what the network is actually learning, making the model more explainalble. It might however learn trends that are not obvious for a human to discern. 



\subsection{hey}
 - Mention feature selectivity
 - comment on featuren not being what we think

\subsection{Link to existing research}
    \comment{I write about the CIFAR-10 results here, but will probably make a more appropriate section. -\Carl} \\
    The results we got on the CIFAR-10 dataset, with the best test accuracy of 51.28\%, was not particularly impressive. It is however known to be a difficult dataset to score well on, with the state-of-the-art convolutional maxout network achieving a test accuracy 88.32\% \citep{Maxout_Networks}, which was the best result of any NN at the time. This was done with a significantly larger network, with convolutional layers, and with more thorough image pre-processing. Channel-out networks have not achieved the same level of accuracy, with a top score of 86.80\% test accuracy \citep{Wang}. This was also our conclusion, with the channel-out network slightly underperforming maxout. However, both channel-out and maxout networks bettered the ordinary ReLU network result by 1.50 pp and 2.09 pp respectively.

    \comment{Maybe include discussion on ML as a tool in neurobiological research \Anna} 


\subsection{Overfitting}
    Throughout this project, we implemented many methods to reduce the chance of overfitting the NNs to the training data. Primary of these have been early stopping, which uses a validation dataset which the network does not train on to monitor whether the cost is improving during the training. As long as the validation cost is improving, we can assume that the network has not yet overfitted the training data, but when the training cost keeps improving and the validation cost goes up, it is a clear sign that the model is starting to overfit.

    L2 penalisation on the kernel weights also helps by penalising situations where the networks learn to delicately balance large weights against each other. This can create results that are stable on the training dataset, but when exposed to inputs not yet seen, the large weights that were balanced on the training inputs can create large fluctuations in the outputs resulting in instability.

    As expected based on the neurobiological phenomenon inhibition of return, dropout works a third regularisation method that ensures that the network does not rely overly much on the activation of certain nodes or on certain input features. In LWTA networks where information is encoded in pathways, it is especially regularising, as it prevents pathways form becoming overly specific to certain trends. If a certain pathway learns how to handle a specific trend at the cost of other pathways learning it, if it is dropped during training, the network will not do well on that trend, and the other pathways are forced to learn it too.

    Finally, LWTA methods can prevent overfitting themselves, as the information is learned locally on subnetworks that are not as complex or capable of learning much information. This means that the subnetworks chosen at inference time are not as prone to being overfit.



\subsubsection{Hyperparameter Tuning}
    An obvious flaw in the tuning of our networks is the lack of tuning of the optimisation algorithm, like the learning rate of the Adam optimiser, or any tuning of dropout and L2 penalisation. We used the common default parameters for the Adam algorithm, and chose dropout rate and L2 penalisation more or less by random with some trial and error. To get better results, these hyperparameters could definitely have been included in the tuning, but this was outside the scope of this project. The main motivation was to see how our neural networks performed with and without regularisation methods added.
    We did find regularisation to improve results on the CIFAR-10 dataset, but there is no clear difference in what kind of network architectures were preferred with or without regularisation. This was somewhat of a surprise, as we did believe regularisation methods would make larger, deeper networks more viable. It did however provide some stability, and allowed for longer training periods, as can be seen from \cref{res:fig:best_LWTA} and \cref{res:fig:best_EPL}.

    The lack of clear results is probably due to the fact that even after tuning, only a rather small part of the parameter space was ever explored. Doing one hyperband iteration with $R=30$, $\eta=3$, as we did with the CIFAR-10 dataset, means only 49 hyperparameter points were explored, whereas there are roughly 1 million available architecture configurations. Without the use of model averaging or any resampling techniques, we can also not be sure that the tuning did not find network architectures that were particularly `lucky' with the initial parameters and thus found a good optimum within the tuning time. On the EPL dataset we could afford a bit more thorough exploration of the architecture hyperparameter space, with three iterations $R=150$, $\eta=3$ we explored a total of 429 hyperparameter points; still not great, but better.
    
    \subsection{Generation of dataset}
        The MNIST and CIFAR-10 datasets was used ``out of the box'' and are thus not relevant for this analysis. On the other hand, the EPL dataset was generated for this purpose, with individual parts of the dataset obtained from a variety of sources.
        
        When concatenating different parts into one big dataset like this, some features are likely to correlate to a large extend. This is not unusual for large dataset, but it is fair to assume that it has affected both the analysis of the data and the PCA. There is also a question as to how the data was sampled and processed by the different sources. Ultimately this lead to a possible incoherence in the dataset which is hard to measure. On the other hand, the features all originated from the same games so, assuming valid sources, they should more or less tell the same story, yielding a reliable dataset. 
    
\subsubsection{PCA}
    The decision to not use PCA for our further analysis of the Premier League data was first and foremost made with respect to \cref{res:fig:explained_variance} where we would need 42 principal components in order to explain 99 \% of the variance. Considering the relatively few features in the original dataset, 87, we could as well perform the analysis directly on the original features. Perhaps if we put the threshold lower than 99 \%, say 80 \%, the principal components would greatly reduce,\footnote{By-eye inspection of the  figure yield about 15 principal components for 80 \% explained variance.} but at the caveat of reduced explained variance. It is fair to assume that PCA would be of great benefit for a larger number of features, as PCA is normally a popular and effective approach when dealing with overfitted models.

    One drawback with regard to the dataset could also be the fact that we composed it ourselves, from a variety of sources, hence yielding some highly correlated features.\footnote{This is because different sources may calculate similar features in different ways. Hence, when they are all included some are bound to correlate} These should probably have been analysed further with a correlation matrix, but this work was not prioritised here, as we quickly discarded the analysis of principal components. Performing correlation analysis would be of greater importance if the dataset contained more features and/or if the nature of the dataset was completely unknown to us in advance. 

    For instance, performing PCA on the MNIST dataset of $28\times 28 = 784$ features would arguably be a good idea. Likewise for the CIFAR-10 dataset of $32\times 32 = 1024$ features. However, this was not done due to the time limit of this project. The analysis of the models trained using these datasets could be performed without PCA while not loosing too much computational efficiency, but a PCA analysis would have been informative in its own rights. 


    

\subsubsection{Explainability of MO/CO}
	\comment{Can features be recognised in the pathways? NB! Features human notice are not necessarily the same as the ones a machine notice -\Anna}


