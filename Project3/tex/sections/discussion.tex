Discussion here, in present/past tense.

\subsection{Interpretation of LWTA Networks}
    \comment{\Anna or \Carl}


\subsection{Link to existing research}
    \comment{I write about the CIFAR-10 results here, but will probably make a more appropriate section. -\Carl} \\
    The results we got on the CIFAR-10 dataset, with the best test accuracy of 51.28\%, was not particularly impressive. It is however known to be a difficult dataset to score well on, with the state-of-the-art convolutional maxout network achieving a test accuracy 88.32\% \citep{Maxout_Networks}, which was the best result of any NN at the time. This was done with a significantly larger network, with convolutional layers, and with more thorough image pre-processing. Channel-out networks have not achieved the same level of accuracy, with a top score of 86.80\% test accuracy \citep{Wang}. This was also our conclusion, with the channel-out network slightly underperforming maxout. However, both channel-out and maxout networks bettered the ordinary ReLU network result by 1.50 pp and 2.09 pp respectively.

    \comment{Maybe include discussion on ML as a tool in neurobiological research \Anna} 


\subsection{Overfitting}
    \comment{I thought it would be fitting (haha) with a separate discussion on overfitting and the various ways in which we have combatted it here. -\Carl} \\
    Throughout this project, we implemented many methods to reduce the chance of overfitting the NNs to the training data. Primary of these have been early stopping, which uses a validation dataset which the network does not train on to monitor whether the cost is improving during the training. As long as the validation cost is improving, we can assume that the network has not yet overfitted the training data, but when the training cost keeps improving and the validation cost goes up, it is a clear sign that the model is starting to overfit.

    L2 penalisation on the kernel weights also helps by penalising situations where the networks learn to delicately balance large weights against each other. This can create results that are stable on the training dataset, but when exposed to inputs not yet seen, the large weights that were balanced on the training inputs can create large fluctuations in the outputs resulting in instability.

    Dropout is a third regularisation method that ensures that the network does not rely overly much on the activation of certain nodes or on certain input features. In LWTA networks where information is encoded in pathways, it is especially regularising, as it prevents pathways form becoming overly specific to certain trends. If a certain pathway learns how to handle a specific trend at the cost of other pathways learning it, if it is dropped during training, the network will not do well on that trend, and the other pathways are forced to learn it too.

    Finally, LWTA methods can prevent overfitting themselves, as the information is learned locally on subnetworks that are not as complex or capable of learning much information. This means that the subnetworks chosen at inference time are not as prone to being overfit.



\subsubsection{Hyperparameter Tuning}
    An obvious flaw in the tuning of our networks is the lack of tuning of the optimisation algorithm, like the learning rate of the Adam optimiser, or any tuning of dropout and L2 penalisation. We used the common default parameters for the Adam algorithm, and chose dropout rate and L2 penalisation more or less by random with some trial and error. To get better results, these hyperparameters could definitely have been included in the tuning, but this was outside the scope of this project. The main motivation was to see how our neural networks performed with and without regularisation methods added.
    We did find regularisation to improve results on the CIFAR-10 dataset, but there is no clear difference in what kind of network architectures were preferred with or without regularisation. This was somewhat of a surprise, as we did believe regularisation methods would make larger, deeper networks more viable. It did however provide some stability, and allowed for longer training periods, as can be seen from \cref{res:fig:best_LWTA} and \cref{res:fig:best_EPL}.

    The lack of clear results is probably due to the fact that even after tuning, only a rather small part of the parameter space was ever explored. Doing one hyperband iteration with $R=30$, $\eta=3$, as we did with the CIFAR-10 dataset, means only 49 hyperparameter points were explored, whereas there are roughly 1 million available architecture configurations. Without the use of model averaging or any resampling techniques, we can also not be sure that the tuning did not find network architectures that were particularly `lucky' with the initial parameters and thus found a good optimum within the tuning time. On the EPL dataset we could afford a bit more thorough exploration of the architecture hyperparameter space, with three iterations $R=150$, $\eta=3$ we explored a total of 429 hyperparameter points; still not great, but better.


\subsubsection{PCA}
    The decision to not use PCA for our further analysis of the premier league data was first and foremost made with respect to figure \ref{res:fig:explained_variance} where we would need 42 principal components in order to explain 99 \% of the variance. Considering the relatively few features in the original dataset, 87, we could as well perform the analysis directly on the original features. Perhaps if we put the threshold lower than 99 \%, say 80 \%, the principal components would greatly reduce (by-eye inspection of the figure yield about 15), but at the caveat of reduced explained variance. It is fair to assume that PCA would be of great benefit for a larger number of features, as PCA is normally a popular and effective approach when dealing with overfitted models. \comment{Add something about the other datasets here? (that they could probably benefit from PCA even though we have not done it)\Johan}.

    One drawback with regard to the dataset could also be the fact that we composed it ourselves, from a variety of sources, hence yielding some highly correlated features. These should probably have been analysed further with a correlation matrix, but this work was not prioritised here, as we quickly discarded the analysis of principal components. Performing correlation analysis would be of greater importance if the dataset contained more features and/or if they were completely unknown to us \comment{hmm bad sentence me thinks, but brian is porridge \Johan}. 

    For instance, performing PCA on the MNIST dataset of $28\cdot 28 = 784$ features would arguably be a good idea. Likewise for the CIFAR-10 dataset of $32\cdot 32 = 1024$ features. However, this was not done due to the time limit of this project. The analysis of the models trained using these datasets could be performed without PCA without loosing too much  computational efficient, but a PCA analysis would have been  informative in its own rights. 
    
    For future work \comment{this should eventually go in the conclusion}:
    \begin{itemize}
        \item Investigate each of the principal components and their dependence on the original features. 
        \item Perform PCA on all datasets. 
        \item Compare the accuracy of trained models using different numbers of principal components, and test this against analysis of the original data. 
    \end{itemize}


    

\subsubsection{Explainability of MO/CO}
    \comment{Can features be recognised in the pathways? NB! Features human notice are not necessarily the same as the ones a machine notice -\Anna}