Theory here, in present tense.

This is an equation
\begin{align} \label{theo:eq:newton2}
    F = ma.
\end{align}

We cite our sources like this~\citep{Project1}.

\subsection{Principal Component Analysis (PCA)}
\comment{not sure where to put this yet (also winging the notation until we meet) -\Johan}
    The motivation behind performing principal component analysis (PCA) is to minimise redundancy of features, by putting emphasis on features which are most \textit{unlike} \comment{need to structure entire section to maximise the sensemaking -\Johan}. The overlap of features is measured as their \textit{covariance}. The goal is to construct a set of new features whose covariance is minimised, which also minimise feature overlapping, then represent the data using these new features in a new feature matrix $Y$. 
    
    \comment{Which feature matrix dimension to use? (m,n) or (n,p). Easy to change, but consistency -\Johan}

    Suppose $X$ is z-score normalised (zero mean) $m\cross n$ matrix representing our data, which consists of $m$ features and $n$ samples. Mathematically, we want to find an $m\cross m$ matrix $P$ such that $PX=Y$ where the covariance matrix of $Y$, $S_Y$, is diagonalised. The covariance matrix of an arbitrary $m\cross n$ matrix $A$ is defined as:

    \begin{align} \label{theo:eq:covariance_definition}
        S_A = \frac{1}{n-1}AA^T.
    \end{align}
    What we effectively do is finding the orthogonal hyperplane in feature space that lies closest to the data. The orthonormal matrix $P$ is then the change of base matrix between the original features spanning $X$, and the newly constructed features that span $Y$. $Y$ can also be interpreted as the projection of the data onto the hyperplane, i.e. the same data represented with a different basis/features which consequently are linear combinations of the original basis/features. 

    Considering the matrix $P$, its rows are \textit{the principal components} of $X$, and it can be shown (\comment{appendix? or leave as an exercise?-\Johan}) that choosing these rows to be eigenvectors of $XX^T$ diagonalises $S_Y$.  The diagonal elements of $S_Y$ are the variances of the projected data. 

    Each row $p_i$ of $P$ represent a new created feature as linear combinations of the original features. As initially stated, we want to maximise the variance, and thus choosing a set of $p_i$ whose variances are above some threshold is the same as selecting the most important non-co-varying features of the data. If we choose $d$ such values the dimensionality of our problem is reduced by $m-d$.
    
    \comment{will link this to the SVD -\Johan}



     

\subsection{Neural Networks}
    First, it will be wise to take a minute to recap some of the details of how a \textit{feed-forward nerual network} (FFNN) is built and trained from~\cite{Project2}. They are made up of sequential \textit{layers} taking in the input from the previous layer and passing it on to the next one. There will always be an input layer, taking in the features $x$ producing some observation $y$, and an output layer transforming the input from the second to last layer into an appropriate output. In classification problems, the outputs will usually be probabilities between $0$ and $1$ for the input to produce an output in a given category. In regression, the output layer usually transforms the output to be a single number $\tilde{y}$ for the predicted value.

    When passing the output from one layer to the next, a linear transformation is done. Denoting the output vector from layer $\ell$ as $\vec{a}^\ell$, the transformation produces a \textit{response} $\vec{z}^{\ell} = W^\ell \vec{a}^{\ell-1} + \vec{b}^\ell$, where $W^\ell$ is a weight matrix, and $\vec{b}^\ell$ is called the bias. The response vector is then passed through an activation function (which is generally non-linear) to produce the \textit{activation} $\vec{a}^\ell = f(\vec{z}^\ell)$.

\subsection{Local Winner Takes All (LWTA)}
    \comment{Explain local learning and sparse pathways -\Anna}
    The idea of LWTA algorithms is to implement local learning in the neural network, such that the entire network does not learn how to transform a given input, but it is rather outsourced to local parts of the network. In LWTA algorithms, sparse pathways learn during training, and these pathways are selected at inference time. The nodes in each layer are grouped into a number of groups $G$, and Winner Takes All is applied on the activation of each of these groups, only passing on the maximal output in the group.

    \subsubsection{Maxout and Channel-Out}
        \comment{Some introductory sentence establishing the excistence of the two algorithms. "There are mainly two algowithms classifying as LWTA; maxout, where pathways are only formed posteriori, and channel-out, where they are also formed anteriori." or something -\Anna}
        The \textit{maxout} activation passes the input from the previous layer through the weight kernel and adds the bias as normal, but then only passes on the activation from the most active node in each group of the layer. Each group then has a common set of weight connections to the next layer in  the network.

        During back propagation in training, the weights of each group are trained according to the activation of the most active node. This can be seen from the gradient of the cost function in the direction of one of the weights of an inactive node: the learning is proportional to $\pd[w^{\ell}_{ij}]{C} \propto \pd[a^\ell_i]{C} = 0$, because any infinitesimal change in the activation would still result in no activation if $a_i$ was not already the maximum of the group.

        \comment{comment on/specify the paths being made due to the weights of the previous layer. See that it is specified below, but I think we could do it here as well. Should be easier when illustrations are included -\Anna}

        % \begin{algorithm}
        %     \caption{Max Out activation}
        %     \begin{algorithmic}[1]
        %         \For{$i=1$ to $g$}
        %             \State $a_i \gets -\infty$
        %             \For{$j=1$ to $n \bmod g$}
        %                 \If{$z_{ij} > a_i$}
        %                     \State $a_i \gets z_{ij}$
        %                 \EndIf
        %             \EndFor
        %         \EndFor
        %     \end{algorithmic}
        % \end{algorithm}

        \textit{Channel-out} makes a subtle, but meaningful, change to the maxout algorithm. Instead of every group having a common set of weight connections to the next layer, every node has its own set of weight connections. This results in not only local learning of the weights during backpropagation, but also leads to the activation winner choosing the set of weights that will be used in the next layer at inference time.
        Effectively, a channel-out layer works the same way as an ordinary dense layer with a linear activation function, but sets all the activations of the nodes that do not win their group to zero before passing them on to the next layer.
        Same as with maxout, only the weights of the active nodes are trained, but this also applies to the next layer, where only weights connected to the active nodes are trained. So the local learning goes both forwards and backwards from a channel-out layer, in contrast to just backwards in the case of maxout.

    \subsection{Neurological Background}
    Winner-takes-all is an important concept in biological neural networks. 
    \comment{Mention maps} \Anna

    \subsubsection{Lateral inhibition}
    A neurological phenomena called lateral inhibition is the primary theoretical justification for LWTA algorithms~\citep{Chen}. 

    
        
