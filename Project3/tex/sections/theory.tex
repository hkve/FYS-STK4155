Theory here, in present tense.

This is an equation
\begin{align} \label{theo:eq:newton2}
    F = ma.
\end{align}

We cite our sources like this~\cite{Project1}.

\subsection{Neural Networks}
    First, it will be wise to take a minute to recap some of the details of how a \textit{feed-forward nerual network} (FFNN) is built and trained from~\cite{Project2}. They are made up of sequential \textit{layers} taking in the input from the previous layer and passing it on to the next one. There will always be an input layer, taking in the features $x$ producing some observation $y$, and an output layer transforming the input from the second to last layer into an appropriate output. In classification problems, the outputs will usually be probabilities between $0$ and $1$ for the input to produce an output in a given category. In regression, the output layer usually transforms the output to be a single number $\tilde{y}$ for the predicted value.

    When passing the output from one layer to the next, a kernel transformation is done. Denoting the output vector from layer $\ell$ as $\vec{a}^\ell$, the transformation produces $\vec{z}^{\ell+1} = W\vec{a}^\ell + \vec{b}^\ell$, where $W$ is a weight matrix, and $\vec{b}$ is called the bias. 


\subsection{Local Winner Takes All}
    The idea of LWTA algorithms is to implement local learning in the neural network, such that the entire network does not learn how to transform a given input, but it is rather outsourced to local parts of the network. In LWTA algorithms, sparse pathways learn during training, and these pathways are selected at inference time. Each layer is grouped into a number of groups $G$, and Winner Takes All is applied on the activation of each of these groups, only passing on the maximal output in the group.

    \subsubsection{Neurological Background}

    \subsubsection{Maxout and Channel-Out}
        The \textit{maxout} activation passes the input from the previous layer through the weight kernel and adds the bias as normal, but then only passes on the activation from the most active node in each group of the layer. Each group then has a common set of weight connections to the next layer in  the network.

        During back propagation in training, the weights of each group are trained according to the activation of the most active node. This can be seen from the gradient of the cost function in the direction of one of the weights of an inactive node: the learning is proportional to $\pd[w^{\ell}_{ij}]{C} \propto \pd[a^\ell_i]{C} = 0$, because any infinitesimal change in the activation would still result in no activation if $a_i$ was not already the maximum of the group.

        % \begin{algorithm}
        %     \caption{Max Out activation}
        %     \begin{algorithmic}[1]
        %         \For{$i=1$ to $g$}
        %             \State $a_i \gets -\infty$
        %             \For{$j=1$ to $n \bmod g$}
        %                 \If{$z_{ij} > a_i$}
        %                     \State $a_i \gets z_{ij}$
        %                 \EndIf
        %             \EndFor
        %         \EndFor
        %     \end{algorithmic}
        % \end{algorithm}

        \textit{Channel-out} makes a subtle, but meaningful, change to the maxout algorithm. Instead of every group having a common set of weight connections to the next layer, every node has its own set of weight connections. This results in not only local learning of the weights during backpropagation, but also leads to the activation winner choosing the set of weights that will be used in the next layer at inference time.
        Effectively, a channel-out layer works the same way as an ordinary dense layer with a linear activation function, but sets all the activations of the nodes that do not win their group to zero before passing them on to the next layer.
        Same as with maxout, only the weights of the active nodes are trained, but this also applies to the next layer, where only weights connected to the active nodes are trained. So the local learning goes both forwards and backwards from a channel-out layer, in contrast to just backwards in the case of maxout.
