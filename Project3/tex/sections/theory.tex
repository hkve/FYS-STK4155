Theory here, in present tense.

This is an equation
\begin{align} \label{theo:eq:newton2}
    F = ma.
\end{align}

We cite our sources like this~\citep{Project1}.

\subsection{Principal Component Analysis (PCA)}
\comment{not sure where to put this yet (also winging the notation until we meet) -\Johan}

\subsection{Neural Networks}
    First, it will be wise to take a minute to recap some of the details of how a \textit{feed-forward nerual network} (FFNN) is built and trained from~\cite{Project2}. They are made up of sequential \textit{layers} taking in the input from the previous layer and passing it on to the next one. There will always be an input layer, taking in the features $x$ producing some observation $y$, and an output layer transforming the input from the second to last layer into an appropriate output. In classification problems, the outputs will usually be probabilities between $0$ and $1$ for the input to produce an output in a given category. In regression, the output layer usually transforms the output to be a single number $\tilde{y}$ for the predicted value.

    When passing the output from one layer to the next, a linear transformation is done. Denoting the output vector from layer $\ell$ as $\vec{a}^\ell$, the transformation produces a \textit{response} $\vec{z}^{\ell} = W^\ell \vec{a}^{\ell-1} + \vec{b}^\ell$, where $W^\ell$ is a weight matrix, and $\vec{b}^\ell$ is called the bias. The response vector is then passed through an activation function (which is generally non-linear) to produce the \textit{activation} $\vec{a}^\ell = f(\vec{z}^\ell)$.


\subsection{Local Winner Takes All (LWTA)}
    \comment{Explain local learning and sparse pathways -\Anna}
    The idea of LWTA algorithms is to implement local learning in the neural network, such that the entire network does not learn how to transform a given input, but it is rather outsourced to local parts of the network. In LWTA algorithms, sparse pathways learn during training, and these pathways are selected at inference time. The nodes in each layer are grouped into a number of groups $G$, and Winner Takes All is applied on the activation of each of these groups, only passing on the maximal output in the group.

    \subsubsection{Maxout and Channel-Out}
        \comment{Some introductory sentence establishing the excistence of the two algorithms. "There are mainly two algowithms classifying as LWTA; maxout, where pathways are only formed posteriori, and channel-out, where they are also formed anteriori." or something -\Anna}
        The \textit{maxout} activation passes the input from the previous layer through the weight kernel and adds the bias as normal, but then only passes on the activation from the most active node in each group of the layer. Each group then has a common set of weight connections to the next layer in  the network.

        During back propagation in training, the weights of each group are trained according to the activation of the most active node. This can be seen from the gradient of the cost function in the direction of one of the weights of an inactive node: the learning is proportional to $\pd[w^{\ell}_{ij}]{C} \propto \pd[a^\ell_i]{C} = 0$, because any infinitesimal change in the activation would still result in no activation if $a_i$ was not already the maximum of the group.

        \comment{comment on/specify the paths being made due to the weights of the previous layer. See that it is specified below, but I think we could do it here as well. Should be easier when illustrations are included -\Anna}

        % \begin{algorithm}
        %     \caption{Max Out activation}
        %     \begin{algorithmic}[1]
        %         \For{$i=1$ to $g$}
        %             \State $a_i \gets -\infty$
        %             \For{$j=1$ to $n \bmod g$}
        %                 \If{$z_{ij} > a_i$}
        %                     \State $a_i \gets z_{ij}$
        %                 \EndIf
        %             \EndFor
        %         \EndFor
        %     \end{algorithmic}
        % \end{algorithm}

        \textit{Channel-out} makes a subtle, but meaningful, change to the maxout algorithm. Instead of every group having a common set of weight connections to the next layer, every node has its own set of weight connections. This results in not only local learning of the weights during backpropagation, but also leads to the activation winner choosing the set of weights that will be used in the next layer at inference time.
        Effectively, a channel-out layer works the same way as an ordinary dense layer with a linear activation function, but sets all the activations of the nodes that do not win their group to zero before passing them on to the next layer.
        Same as with maxout, only the weights of the active nodes are trained, but this also applies to the next layer, where only weights connected to the active nodes are trained. So the local learning goes both forwards and backwards from a channel-out layer, in contrast to just backwards in the case of maxout.

    \subsection{Neurological Background}
    Winner-takes-all is an important concept in biological neural networks. 
    \comment{Mention maps} \Anna

    \subsubsection{Lateral inhibition}
    A neurological phenomena called lateral inhibition is the primary theoretical justification for LWTA algorithms~\citep{Chen}. 

    
        
