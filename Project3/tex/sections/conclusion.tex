Conclusion here, in past/present/future tenses.

Looking at how information is handled in biological processes provides an interesting angle in guiding machine learning algorithms. The concept of winner-takes-all behaviour and local learning does have an obvious parallel that can be exploited in neural networks. We found effective implementations of LWTA behaviour with two types of networks; maxout and channel-out. These networks encode information in local pathways, creating subnetworks that are chosen at inference time. We found this to help with the explainability of notoriously opaque neural networks.

Applying the network on the benchmark image recognition dataset CIFAR-10, we found a test accuracy of 51.28\% with maxout layers with dropout, and 50.69\% with channel-out layers with ridge penalisation. This was an improvement on an ordinary Dense neural network with ReLU activation, achieving 49.19\% test accuracy. Applying to our Premier League dataset from the 2019/2020 season, we found a test accuracy in predicting match outcomes of 58.06\% with maxout layers with dropout and ridge penalisation and 54.84\% with purely channel-out layers. This did not improve on the results from an ordinary dense ReLU network achieving 58.87\% test accuracy.

These results were achieved after tuning the network architecture with the hyperband search algorithm, and we believe it could be improved with tuning of more hyperparameters, and if with more computational resources applied to the tuning search. A natural next step to take would be to include convolutional LWTA layers on the image recognition data, and reduce the features using max pooling layers. On the EPL data, it would be natural to expand the dataset to include more statistics from more than one prior match, and use recurrent neural networks to allow them to get a better sense of the form trends in the team performances.

From our results, it is not clear whether LWTA networks are useful in and of themselves, other than providing more transparency in their inner workings. A natural application could perhaps be to model neurobiological systems to study how these systems encode and infer information. It would also be interesting to look at more complete neurological network models, like those of spiking neural networks or recurrent neural networks that implement a time component to the network \citep{Chen}. \comment{\Anna, help? -\Carl}

The principal component analysis did not prove fruitful enough to be included in the final analysis of the EPL data, due to the relatively few original features, and limited time. With more time however, this would have been improved and implemented, both on the EPL dataset, but also on the MNIST and CIFAR-10 datasets. Thus, future work might include a more thorough PCA, comparing the accuracy of trained models using different numbers of principal components. An interesting analysis could also be to investigate the principal components' dependence on the original features. 


\comment{Frampek til RNN \& SpNN (would perhaps be more biologically "correct"; time dependent) \Carl \Anna}


\comment{EPL anal: Would be cool to look back more games than one}