\documentclass[twocolumn,english,notitlepage]{article}
\usepackage[margin=2cm]{geometry}

% \setlength{\parindent}{0pt} % no indents

\usepackage{overhead} % Overhead is in a separate file

\title{Project 3}
\author{Anna Hjertvik Dåsen, Haakon Kværnmoen, et al.}

\date{\today}

\begin{document}

\twocolumn[
    \begin{@twocolumnfalse}
        \maketitle
        \begin{abstract}
            We integrated two LWTA algorithms, maxout and channel-out, with tensorflow neural networks and applied them to the MNIST and CIFAR-10 image recognition datasets. We visualised how these algorithms train specific pathways in its architecture, providing an opportunity to discern how information is encoded in the neural networks. A maxout neural network including dropout achieved a test accuracy of 51.28\% on the CIFAR-10 dataset, and a test accuracy of 50.69\% with a channel-out network with L2 weight penalisation.

            Furthermore, we created and analysed a dataset comprised of statistics from English Premier League football matches, and applied LWTA NNs in classifying results of the final matches 124 matches of the 2019/2020 season. With a maxout NN with dropout and L2 weight penalisation we got an accuracy of 58.06\%, and with a plain channel-out NN we got 54.84\%. Neither of these beat an ordinary dense NN with ReLU activation achieving an accuracy of 58.87\%. Here goes PCA results.
        \end{abstract}
    \end{@twocolumnfalse}
]

\tableofcontents

\section{Introduction}
\input{sections/introduction}

\section{Theory}
\input{sections/theory}

\section{Method}
\input{sections/method}

\section{Results}
\input{sections/results}

\section{Discussion}
\input{sections/discussion}

\section{Concluding remarks} 
\input{sections/conclusion}


% for numbering appendix equations more appropriately
\numberwithin{equation}{section}
\renewcommand{\theequation}{\thesection.\arabic{equation}}
\newpage
\begin{appendices}
    \input{sections/appendix} 
\end{appendices}

% \printbibliography
\bibliography{refs/references}

\end{document}

