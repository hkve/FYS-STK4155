\comment{Suggestion: present tense, makes sense but might mess up some times H}\\
In this project, I thought we could use \verb|cleveref| to reference things like \cref{int:fig:example}.


Before we delve into to details of the theory section, it is useful to take a moment to define the quantities we will be dealing with. Let $\vec{y}$ denote a vector of a series of measured values $y_i$, $i\in 1, 2,\ldots, n$ at points $X$, where $X$ is a matrix where the rows $x_i$ correspond to the input values that produced the measurement $y_i$. For completeness, we refer to the columns of $X$ as $\vec{x}_a$, $a=1, 2,\ldots, p$.\footnote{Throughout this project, $a, b$ will always index the $p$ features for a model, and $i, j$ the $n$ data points.} Together, these form the dataset $\mathcal{D} = \cclosed{(y_i, x_i)}$. Another important set of values is $\vec{\theta}$ which contains the parameters that will define the model we will use to predict the data.

We will assume that the measurements $y_i$ are generated from an exact function $f(x)$ with stochastic noise $\epsilon$ added, such that $y_i = f(x_i) + \epsilon$. Throughout this project, we will assume that the noise to be distributed normally with zero mean and some standard deviation (std) $\sigma$; $\epsilon \distas \normal{0}{\sigma^2}$. Furthermore, we will assume that the noise between different measurements is independent and identically distributed (i.i.d.). Our job then is to model an $\hat{f}(x)$ that we want to be close to $f(x)$.
\comment{Copied this section from previous project, as it will be useful here too. Needs rewriting, -\Carl}
\comment{Also add somewhere: } \cite{Project1}.
\subsection{Gradient Descent Methods}
    \comment{This section might well be moved to method, it is not too theoretical. -\Carl}\\
    Optimisation of the parameters of the models we will be employing in this project is formulated as minimisation problems, with an associated cost function $C(\vec{\theta})$ to be minimised to find the optimal parameters.
    \subsubsection{Plain Gradient Descent}
        We will compare different variants of GD algorithm, starting first with the plain one. Assume the gradient of the cost function $C(\vec{\theta})$ w.r.t. the parameters $\vec{\theta}$ is well-defined on the entirety of our parameter space, and is calculable either analytically or approximated numerically\footnote{Now this is not always the case, e.g. with $L_1$-penalisation on the parameters.}. The idea is that the fastest direction to move in parameter space to reduce $C(\vec{\theta})$ is to follow the direction of $-\Nablatheta C(\vec{\theta})$. Plain gradient descent implements this idea iteratively, starting from an initial guess $\vec{\theta}_0$, finding the next point by
        \begin{align*}
            \vec{\theta}_{k} = \vec{\theta}_{k-1} - \eta \Nablatheta C(\vec{\theta}_{k-1}),
        \end{align*}
        where $k=1,2,\ldots,N$ for $N$ iterations, and $\eta$ is an introduced hyperparameter called the learning rate.

    \subsubsection{Adding Momentum}
        The simple GD can be fleshed out by giving the algorithm a sense of inertia, helping it continue to move in a certain direction. This combats tendency to oscillate around saddle points, for instance. It starts as earlier, but with the additional initialisation $\vec{p}_0 = 0$, finding $\vec{\theta}_k$ iteratively by 
        \begin{subequations}
            \begin{align}
                \vec{p}_{k} &= \gamma \vec{p}_{k-1} + \eta \Nablatheta C(\vec{\theta}_{k-1}), \\
                \vec{\theta}_k &= \vec{\theta}_{k-1} - \vec{p}_k,
            \end{align}
        \end{subequations}
        where $\gamma$ is a new hyperparameter characterising the memory of earlier gradients.

    \subsubsection{Stochastic Gradient Descent} 
        Stochastic Gradient Descent (SGD) is based on functions to minimise that are on the form $C(\vec{\theta}) = \sum_{i \in A} c_i(\vec{\theta}) + c_0(\vec{\theta})$ for $A=\cclosed{1,2,\ldots,n}$. This is often the case, as in MSE-scores, where a cost is associated with every observation $y_i$ separately. The idea of SGD is to avoid computing the gradient of the whole of $C$, but rather split it into mini-batches $C_k(\vec{\theta}) = \sum_{i\in A_k} c_i(\vec{\theta}) + c_0(\vec{\theta})$ for some subset $A_k \subseteq A$, and compute the gradient only on one of these smaller batches. This helps solve two problems: First, it makes the computational cost of calculating the gradient smaller, as we no longer compute the full one, and second, it adds a degree of stochasticity to the movement in parameter space since we no longer will follow the `true' gradient, which prevents getting stuck in local minima.

        SGD is done in so-called epochs, where we divide the full set $A$ into equally sized batches $A_k$, and do one GD iteration on every batch once. After this, $A$ is reshuffled and new mini-batches are drawn.

    \subsubsection{Improving Convergence Rate}
        \comment{Talk about learning schedules.}\\
        As stochasticity is added to the algorithm, the rate of convergence does slow down. As such, it would be nice to add algorithms that can adapt the learning rate $\eta$ in such a way as to increase the rate of convergence. The AdaGrad algorithm does this with the idea to use different learning rates along different directions in parameter space; i.e. use larger learning rates in flat directions, while keeping it small in steep directions. This is done by keeping a running sum $G_k$ of the outer product of the gradients $\vec{g}_k$ at the points $\vec{\theta}_k$, and use this to adapt the learning rate.
        \comment{I don't know whether to include the update rules in the \LaTeX-document, or maybe move them to method, or whatever. -\Carl}
        \begin{subequations}
            \begin{align}
                \vec{g}_k &= \Nablatheta C_k(\vec{\theta}_{k-1}) \\
                G_k &= G_{k-1} + \vec{g}_j \otimes \vec{g}_j \\
                \vec{\theta}_k &= \vec{\theta}_{k-1} - \eta G_k^{-\sfrac{1}{2}} \vec{g}_k
            \end{align}
        \end{subequations}
        where $\vec{v} \otimes \vec{u}$ denotes the outer product of two vectors $\vec{v}, \vec{u}$, and a matrix $m = M^{-1/2}$ is understood to the matrix satisfying $m^2 = M^{-1}$. Inverting and finding the root of $G_k$ comes at a computational cost, so we will employ the much cheaper version where we update \comment{Discussion note: mention uncorrelated features bettering this approximation, perhaps. -\Carl}
        \begin{align}
            \vec{\theta}_{k} = \vec{\theta}_{k-1} - \frac{\eta}{\sqrt{\diag(G_k)}+\epsilon} \circ \vec{g}_k,
        \end{align}
        where the arithmetic operations are understood to be element-by-element and $\vec{v}\circ \vec{u}$ represents the element-wise multiplication of two vectors $\vec{v}, \vec{u}$, and $\epsilon$ is a small number to avoid zero-division.
        
        The AdaGrad algorithm serves to tune the learning rate, and can be applied to gradient descent with momentum too. This gives the update rule
        \begin{subequations}
            \begin{align}
                \vec{g}_k &= \Nablatheta C_k(\vec{\theta}_{k-1}) \\
                \vec{p}_k &= \gamma \vec{p}_{k-1} + \eta \vec{g} \\
                G_k &= G_{k-1} + \vec{g}_j \otimes \vec{g}_j \\
                \vec{\theta}_k &= \vec{\theta}_{k-1} - \frac{\eta}{\sqrt{\diag(G_k)}+\epsilon} \circ \vec{p}_k
            \end{align}
        \end{subequations}

        Another way to adapt the learning rate to the parameter space `terrain' is found in the Root Mean Square Propagation (RMSprop) algorithm. Here we approximate a running average of the second moment of the gradient $\vec{s} = \expval(\vec{g} \circ \vec{g})$, then use this to scale the learning rate appropriately.
        \begin{subequations}
            \begin{align}
                \vec{g}_k &= \Nablatheta C_k(\vec{\theta}_{k-1}) \\
                \vec{s}_k &= \beta \vec{s}_{k-1} + (1-\beta) \vec{g}_k \circ \vec{g}_k \\
                \vec{\theta}_k &= \vec{\theta}_{k-1} - \frac{\eta}{\sqrt{\vec{s}_k}+\epsilon} \circ \vec{g}_k
            \end{align}
        \end{subequations}
        where again $\epsilon$ is a small number to avoid zero-division, and the arithmetic operations are understood to be element-wise. Here $\beta \in [0,1]$ is a new hyperparameter parametrising the rate at which previous gradients should inform the current average of the second moment.

        Finally, an algorithm which aims to combine RMSprop with momentum is Adam. Here we also keep a running average of $\vec{m} = \expval(\vec{g})$, i.e. the first moment of the gradient. Moreover, bias-corrected values are used in lieu of the `bare' estimates of the first and second moments, here denoted with hats. This ensures that a bias for low values of $\vec{m}_k, \vec{s}_k$ in early iterations with high $\beta$-values is avoided.
        \begin{subequations}
            \begin{align}
                \vec{g}_k &= \Nablatheta C_k(\vec{\theta}_{k-1}) \\
                \vec{m}_k &= \beta_1 \vec{m}_{k-1} + (1-\beta_1) \vec{g}_k \\
                \vec{s}_k &= \beta_2 \vec{s}_{k-1} + (1-\beta_2) \vec{g}_k \circ \vec{g}_k \\
                \vec{\hat{m}}_k &= \frac{\vec{m}_k}{1-\beta_1^k} \\
                \vec{\hat{s}}_k &= \frac{\vec{s}_k}{1-\beta_2^k} \\
                \vec{\theta}_k &= \vec{\theta}_{k-1} - \frac{\eta}{\sqrt{\vec{\hat{s}}_k}+\epsilon} \circ \vec{\hat{m}}_k
            \end{align}
        \end{subequations}


        \comment{Discussion notes: Adagrad accumulates G such that the learning rate becomes smaller with iterations --- AKA GD is `turned off'. -\Carl \\
        Not entirely true, rather learning rate stabilizes, I think. -\Carl}

    \subsection{Neural Networks}
        Artificial neural networks (ANN) are biologically inspired machine learning algoritms which can improve performance as they are exposed to data and without being programmed with any task-specific rules. In this project fully connected \textit{feed-forward neural networks (FFNN)} will be implemented. 
        \subsubsection{Structure}
        First a non-mathematical, intuitive introduction. 
        An ANN consists of an input layer and an output layer. Often the network will also have layers betweeen the input and output, called \textit{hidden layers}. In that case the neural network is referred to as deep. Each layer contains \textit{nodes} which are supposed to represent the neurons in a biological neural network, or rather their activation. In a fully connected neural network, all nodes in one layer are connected to all the nodes in the previous as well as the next layer. These connections, which represent the synapses, are referred to as \textit{weights}, and the sizes of the weights are the synaptic strength. Each node also have an activation \textit{bias} which says something about how easily activated that specific neuron/node is. See \cref{the:fig:Illustration_NN} for an illustration of a simple, deep artificial neural network. 
        \begin{figure} [ht]
            \centering
            \includegraphics[width=.85\linewidth]{figs/Illustration_NN.png}
            \caption{Illustration of a simple deep neural network with an input layer, two hidden layer, and an output layer. The circles are the nodes and the lines between are the weights.}
            \label[fig]{the:fig:Illustration_NN}
        \end{figure}
        
        The nodes, weights and biases are all essentially just numbers, but following an algorithm for combinig these numbers creates an ANN. 

        \subsubsection{Feed Forward}
            \begin{figure} [ht]
                \centering
                \includegraphics[width=.4\linewidth]{figs/Illustration_simple.png}
                \caption{Illustration of a simple neural network with an input layer, and an output layer consisting of one node.}
                \label[fig]{the:fig:simple_NN}
            \end{figure}
            The simplest of neural networks is the \textit{Feed Forward Neural Network(FFNN)} which, as its name implies, feeds activation from the input layer and forward through the network, eventually ending up in the output layer. Given a NN with only an input layer, and an output layer consisting of one node (see \cref{the:fig:simple_NN}), the output $y$ is then  
            \begin{equation}
                y=f\left(\sum_{i=1}^n w_i x_i+b\right)
            \label[eq]{the:eq:output}
            \end{equation} 
            where $x_i$ constitutes the input, $w_i$ are the weights corresponding to each input variable, and $b$ the bias of the output node. $f$ is called the \textit{activation function} and depends on the analysis being executed; e.g. regression, classification, etc. Each node has a corresponding activation function which defines the possible outputs of that node. The activation function closest to replicating the behaviour of a biological neuron is the heaviside function which yields either $0$ or $1$; firing or not firing. However, there are in some cases advantages of abandoning this biological model. Some other, and currently more used activation functions are: linear, sigmoid, relu, tanh, and leaky relu some of which are plotted in \cref{the:fig:introducing_acts}. \comment{See more on activation functions in appendix ref} 
            \begin{figure} [ht]
                \centering
                \includegraphics[width=\linewidth]{figs/introducing_acts.pdf}
                \caption{Plots of some commonly used activation functions.}
                \label[fig]{the:fig:introducing_acts}
            \end{figure}
            
            In the output layer the activation function constitutes what analysis the network does. E.g. a form of binary classification can be done by implementing the sigmoid activation function in the output layer, and for regression problems the linear function, $f(x)=x$, is implemented.
            It is normal that all the nodes in a layer have the same activation function $f$.    

            If we were to have a more complex neural network than the one from \cref{the:fig:simple_NN}, now also including hidden layers, $y$ (ref \cref{the:eq:output}) would be a node in the layer after the input. Each such node would have its own activation bias and would be connected with the input, or activation from the previous layer, through "unique" weights.  
        

        \subsubsection{Backpropagation}

    \subsubsection{Universal Approximation Theorem}

\subsection{Logistic regression}
    For comparison with our neural network we introduce maybe the most common classification method there is, \textit{logistic regression}. One might be confused by inclusion of the word \textit{regression} in the name of a classification method. This comes from preforming a fit to the \textit{log-odds} using a function linear in the input variables:
    \begin{align*}
        P(\vec{y}|X,\vec{\vec{\theta}}) = \prod_{i = 1}^{n} \sigma(X_{ia} \theta_a)^{y_i}[1-\sigma(X_{ia}\theta_a)]^{1 - y_i}
    \end{align*}

    \begin{align*}
        \log(\frac{p_{y_i}(x_i)}{1-p_{y_i}(x_i)}) = x_i^T \theta
    \end{align*}

    \begin{align*}
        P(\vec{y}|X,\vec{\vec{\theta}}) = \prod_{i = 1}^{n} \sigma(X_{ia} \theta_a)^{y_i}[1-\sigma(X_{ia}\theta_a)]^{1 - y_i} 
    \end{align*}

    \begin{align*}
        \msub{C}{BCE}(\vec{\theta}) &= -\log P(\vec{y}|X,\vec{\vec{\theta}}) \\ 
        &= - \sum_{i = 1}^{n} y_i \sigma(X_{ia} \theta_a) + (1 - y_i)[1-\sigma(X_{ia}\theta_a)]
    \end{align*}

    \begin{align*}
        (\grad_{\theta} \msub{C}{BCE})_b &= \sum_{i = 1}^{n} y_i [1 - \sigma(X_{ia} \theta_a) ] X_{ib} + -(1 - y_i)\sigma(X_{ia} \theta_a)X_{ib} \\
        &= \sum_i^n (y_i - \sigma(X_{ia}\theta_a)) X_{ib}  
        \pdv{\theta_b} \msub{C}{BCE} \\
        &= \sum_{i = 1}^{n} \left( (1 - y_i)\sigma(X_{ia} \theta_a) - y_i [1 - \sigma(X_{ia} \theta_a) ] \right) X_{ib} \\
        &= \sum_i^n (\sigma(X_{ia}\theta_a)- y_i) X_{ib}  
    \end{align*}

    \begin{align*}
        \nabla_{\vec{\theta}} \msub{C}{BCE}(\vec{\theta}) = X^T (\hat{\vec{p}} - \vec{y}) 
    \end{align*}