\comment{Suggestion: past tense}\\

We have introduced and implemented a wide range of GD and SGD algorithms, and found that they perform satisfactorily when minimising OLS and ridge cost functions of either linear models, logistic models or neural networks with up to many thousands of parameters. Minimisation of linear models was done on the Franke function data with stochastic noise, using a polynomial expansion of degree 5 of the two arguments. Fitting to 600 points split into train- and test portions with a $\sfrac{3}{4}$ split, we found that adding momentum to the GD algorithm increased convergence rate and decreased sensitivity to initialisation, but decreased stability. Adding stochasticity did much the same, but using tunable learning rates as with the AdaGrad, RMSprop and Adam algorithms increased stability. Overall, the momentum based SGD with Adam performed the best with a validation MSE of $0.1660 \pm 0.0018$ using the OLS cost function with 500 epochs and a batch size of 200.

This led us to employ the Adam when training our NNs both for regression of Nicaraguan terrain data similar to the Franke data, and classification of breast tumours from the Wisconsin breast cancer dataset. In the regression case, we compared our results with those found from linear regression based on methods from \cite{Project1}, expanding the two arguments of the terrain data polynomially to degree 11 and analytically finding the optimum parameters with an OLS cost function. Using a NN with 5 hidden layers with 200 nodes each, and training on 450 of the total 600 data points with 500 epochs of SGD with Adam and a batch size of 200, we got a validation MSE of $0.078$ compared with $0.171$ from the linear model. With a network of this size, we found that adding a penalisation on the size of the weights to improve the results, using an $L_2$ ridge penalisation parameter $\lambda = 10^{-4}$.

We tried using different activation functions for the hidden layers of our network, both for regression and classification, and found that the sigmoid activation function gave us the best results in both cases, with the ReLU functions close behind for regression and $\tanh$ for classification.

Classification\ldots