\subsection{Gradient Descent Methods}
    The GD methods we employed were not especially impressive in optimising our linear regression parameters when compared to the analytical solution obtained from matrix inversion. However, we used a pretty limited dataset, and did not use that many features (20 in total). Matrix inversion will quickly become computationally expensive, and can also become unstable when there is correlation between the features. Some notes on the different algorithms employed follow.

    We did not analyse fully the role of epochs and batch size with the stochastic methods, focusing rather on the more overarching differences between stochastic and non-stochastic methods, together with the differences between algorithms. To obtain better results in our optimisation, looking closer at the tuning of these parameters would have been helpful.

    \subsubsection{Ordinary Gradient Descent}
        The non-stochastic GD was characterised by being sensitive to initialisation, but performed well on our simple optimisation problems when momentum was added, both on the OLS and ridge cost functions. These algorithms were faster than their stochastic counterpart, largely due to the fact that they update the parameters less often. It is clear that ordinary GD can be powerful when the parameter terrain is sufficiently non-complex, or if you have the computational resources to initialise multiple times. 

    \subsubsection{Stochastic Gradient Descent}
        Adding stochasticity to the GD methods seemed to make them converge faster, at the expense of less stability. This is likely due to not being caught up in local minima, something both stochasticity and momentum can help with. Thus, it makes sense that the gap in performance between plain SGD and SGD with momentum was smaller (MSEs of 0.1862 and 0.1729 respectively) compared to the `bare' GD (MSEs of 0.1982 and 0.1775 respectively). However, one of the significant improvements was the improvement in sensitivity to initialisation, which makes sense seeing as the stochastic algorithms can escape the pull of the local parameter space terrain more easily to venture towards greater minima.

    \subsubsection{AdaGrad methods}
        With the AdaGrad algorithms, the diagonal of $G$ accumulates every iteration, adding $g_i^2$, meaning that the learning rates $\eta \propto G^{-\sfrac{1}{2}}$ will only decrease. This results in AdaGrad `turning off' the descent, effectively implementing a cap to the distance it will travel in parameter space. This in turn makes it very safe, combating exploding gradients very effectively, as evidenced in \cref{res:fig:lrate3}. However, it also makes it more sensitive to the initialisation of the parameters: If it starts far from a `good' minimum in parameter space, it will struggle to reach it before turning off.

    \subsubsection{RMSprop and Adam}
        Both RMSprop and Adam married some of the stability of the tunable AdaGrad across learning rates, but managed to do so without turning off the learning rate, but rather using running averages $\vec{s}_k$ that do not monotonically increase. This meant that they were significantly less sensitive to the initialisation, as they managed to converge regardless. Again, adding momentum as with Adam increased stability and convergence. 

        
        \subsubsection{Ridge Penalisation on Linear Regression}
        Adding a ridge penalisation on the parameters of our linear regression optimisation seemed to have little to no effect. We might attribute this to the fact that our linear model was not complex, and the data was `nice' enough, as to not be prone to overfitting. Essentially, the degrees of freedom in our fit seemed appropriate, and did not need penalisation. It was interesting to note that it did decrease the sensitivity to the initialisation of the GD methods, however.


\subsection{Neural Networks as Regressors}
    \subsubsection{Initialisation of parameters}
        As the data is scaled such that the features and data all have mean zero and std one, initialising the weights with the same distribution ensures that they are all of the same order of magnitude. Thus, it is safe to assume that the weights are initialised without too much bias to away from the area of parameter space that should be of interest. We could have explored other ways of initialising the parameters of the network, like initialising with different distributions, or changing the std, and see how the network performed differently. Perhaps different architectures would do better when initialised in different ways. 


    \subsubsection{Network Architecture}
        We saw that the optimal network with sigmoid activation function in the hidden layers was our implementation of \network{5}{200} with learning rate 0.001. This was the most complex network we tested, meaning we might get even better results with by further increasing the complexity. However, the universal approximation theorem tells us that for the appropriate network and training we can perfectly reproduce the training data, leading to overfitting. This network had 161 601 parameters (160 600 weights and 1001 biases), meaning it is enormous compared to the data we trained on, and could be prone to overfitting. The network did best with lower learning rates, so the fact that it avoided overfitting so well might well be due to the network not training enough. Furthermore, there is no actual noise added to the data, so the training and validation data may be quite similar, which can diminish the problem of overfitting. The data we sent in had two input variables, and we sent in 450 datapoints in the training process.
        
        We saw that our and the \verb|scikit-learn|-model had similar performances, implying our implementation is consistent with the norm.

    \subsubsection{Choice of Hidden Activation Function}
        It is interesting to note that the shape of a predicted graph in some sense inherits the shape of the activation function. The reason the network with the identity activation is linear, is that no non-linearity is ever introduced, so the output can only ever be linear in the input arguments.


\subsection{Classification, Wisconsin Breast Cancer}
    Both Logistic regression and our Neural network managed to achieve a very good accuracy score calculated on the test set. The entire data set contained 569 unique instances, and by using our train-test split of $\sfrac{3}{4}$, the test set contained 143 points. Looking back at \cref{res:tab:accuracy_score_activation_functions}, we found optimal validation accuracies of $0.933$ for many models, using both different network structures and hyperparameters. This corresponds to a single instance being wrongly classified, which one might consider a highly effective network. However, the Wisconsin Breast Cancer data set is known to be ideal and 'noise free' \citep{easyCancer}. Therefore, having a repertoire of good performing models is not groundbreaking. For further analysis, the usage of lower train-test split ratios would be interesting. Model selection and hyperparameter optimisation could be performed as a function of train set size, which could indicate how little training data would be necessary to achieve good results. 
    
    Every feature from the data set has been included for all fitted models. This might be an over complication of the model, where the decisive factors could be a subset of the features present in the data set. Usage of some feature selection techniques would help to reduce the complexity of the model. In \citep{inproceedings}, principal component analysis (PCA) was performed on the Wisconsin Breast Cancer, where they found that the number of features could be reduced to 15 covering 99.6\% explained variance, by making linear combinations of the original features.

    The accuracy score has consistently been used to evaluate models, but looking at the number of false positives (FP) and false negatives (FN) might also be an interesting metric to use. Distinguishing between the type of misclassification could help to narrow down the possible models, where an FN would be worse than an FP.


    \subsubsection{Logistic Regression}
        When investigating the effect of different optimisation algorithms, Adam SGD and mGD achieved a high accuracy for the least amount of iterations, as seen in \cref{res:fig:logreg_under_optimazation}, both yielding a validation accuracy of $0.986$ after about 25 iterations/epochs. When penalisation was added \cref{res:fig:logreg_penalisation}, no increase of validation accuracy was achieved, but two different classes of models performed equally good. For a weak penalisation, $\lambda = 10^{-6}$ the learning rate of $\eta = 0.13$ was found, resembling the model without penalisation. Using the higher penalisation of $\lambda = 10^{-1.5}$, the $0.985$ validation accuracy was achieved for a bigger range of learning rates $\eta \in [0.25, 0.5]$. Despite such high learning rates, no exploding gradients or large variation in validation accuracies was found, indicating that the parameters converges to a good minimum. The $L_2$ regularisation also helps to set non-influential weights small, reducing the complexity of the model.


    \subsubsection{Neural Network}
        The validation accuracies from \cref{res:tab:act_funcs} yielded good results across structures and activation functions. The \network{1}{5} structure using the sigmoid hidden activation function achieved the most stable 0.993 accuracy across all the structures, classifying with a single mistake for $\eta \in [0.15, 0.27]$ as seen in \cref{res:fig:clf_af_N_1_5}. When doubling the number of nodes in this single layer (\network{1}{10}, \cref{res:fig:clf_af_N_1_10}), performance decreased for the learning rates tried here. When increasing to \network{2}{5} (\cref{res:fig:clf_af_N_2_5}) and \network{3}{5} (\cref{res:fig:clf_af_N_3_5}), the preferred hidden activation functions was no longer as clear, without increasing performance or accuracy. Adding penalisation to \network{1}{5} and \network{2}{5} using the sigmoid and hyperbolic tangent seen in figure \cref{res:fig:clf_heatmaps}, lower penalisation was preferred. Using \network{1}{5} and \network{2}{5} the sigmoid function achieved an accuracy of 0.993 for multiple penalisation values  $\lambda \in [10^{-8.0}, 10^{-3.5}]$ and $\lambda \in [10^{-8.0}, 10^{-5.75}]$ respectively. The same structures using the hyperbolic tangent function achieved fewer optimal values, where \network{1}{5} had a lower accuracy score of 0.986. Using \verb|scikit-learn|'s neural network implementation, the best validation accuracy scores achieved was 0.986 as seen in figure \cref{res:fig:clf_heatmap_sklearn}. Having multiple optimal scores in the vicinity of the results obtained using our implementation, serves as a good benchmarking case validating that our neural network implementation is correct. The since the \verb|scikit-learn| penalisation parameter $\alpha$ is not equal to our $\lambda$, direct comparisons between penalisation term is not feasible.


        When comparing logistic regression with the neural network models, we found no significant benefit of using the more complex neural network with the approach applied in this work. In a real world scenario, understanding how the model classifies and how different features influences the result might be more important than a slight increase in accuracy. By directly looking at coefficient sizes, logistic regression is more easily interpreted, especially when compared to multi layered neural networks.  
        
        
\subsection{Model Selection}
    Model selection is prevalent in all parts of this project, and in many cases, the difference between models has been marginal. To get a more thorough examination of which GD methods, regressors and classifiers work best, it would have been helpful to employ resampling techniques like bootstrap or cross-validation, as in~\citep{Project1}, to get better estimates for the scores we have employed throughout, e.g. MSE, R2 and accuracy.