\comment{Suggestion: present/past tense}\\

\subsection{Gradient Descent Methods}
    The GD methods we employed were not especially impressive in optimising our linear regression parameters when compared to the analytical solution obtained from matrix inversion. However, we used a pretty limited dataset, and did not use that many features (20 in total). Matrix inversion will quickly become computationally expensive, and can also become unstable when there is correlation between the features. Some notes on the different algorithms employed follow.

    We did not analyse fully the role of epochs and batch size with the stochastic methods, focusing rather on the more overarching differences between stochastic and non-stochastic methods, together with the differences between algorithms. To obtain better results in our optimisation, looking closer at the tuning of these parameters would have been helpful.

    \subsubsection{Ordinary Gradient Descent}
        The non-stochastic GD was characterised by being sensitive to initialisation, but performed well on our simple optimisation problems when momentum was added, both on the OLS and ridge cost functions. These algorithms were faster than their stochastic counterpart, largely due to the fact that they update the parameters less often. It is clear that ordinary GD can be powerful when the parameter terrain is sufficiently non-complex, or if you have the computational resources to initialise multiple times. 

    \subsubsection{Stochastic Gradient Descent}
        Adding stochasticity to the GD methods seemed to make them converge faster, at the expense of less stability. This is likely due to not being caught up in local minima, something both stochasticity and momentum can help with. Thus, it makes sense that the gap in performance between plain SGD and SGD with momentum was smaller (MSEs of 0.1862 and 0.1729 respectively) compared to the `bare' GD (MSEs of 0.1982 and 0.1775 respectively). However, one of the significant improvements was the improvement in sensitivity to initialisation, which makes sense seeing as the stochastic algorithms can escape the pull of the local parameter space terrain more easily to venture towards greater minima.

    \subsubsection{AdaGrad methods}
        With the AdaGrad algorithms, the diagonal of $G$ accumulates every iteration, adding $g_i^2$, meaning that the learning rates $\eta \propto G^{-\sfrac{1}{2}}$ will only decrease. This results in AdaGrad `turning off' the descent, effectively implementing a cap to the distance it will travel in parameter space. This in turn makes it very safe, combating exploding gradients very effectively, as evidenced in \cref{res:fig:lrate3}. However, it also makes it more sensitive to the initialisation of the parameters: If it starts far from a `good' minimum in parameter space, it will struggle to reach it before turning off.

    \subsubsection{RMSprop and Adam}
        Both RMSprop and Adam married some of the stability of the tunable AdaGrad across learning rates, but managed to do so without turning off the learning rate, but rather using running averages $\vec{s}_k$ that do not monotonically increase. This meant that they were significantly less sensitive to the initialisation, as they managed to converge regardless. Again, adding momentum as with Adam increased stability and convergence. 

        
        \subsubsection{Ridge Penalisation on Linear Regression}
        Adding a ridge penalisation on the parameters of our linear regression optimisation seemed to have little to no effect. We might attribute this to the fact that our linear model was not complex, and the data was `nice' enough, as to not be prone to overfitting. Essentially, the degrees of freedom in our fit seemed appropriate, and did not need penalisation. It was interesting to note that it did decrease the sensitivity to the initialisation of the GD methods, however.

    \subsection{Neural Networks and regression}
        \subsubsection{Initialisation of parameters}
            With our scaling of the data, every feature with mean at zero and standard deviation equals $1$, the initialisation of the weights with the same distribution intuitively makes sense. \comment{please rewrite in a more academic fashion}
            We could have explored other ways of initialising the parameters of the network. 
    

        \subsubsection{The choice of epochs}
            Could hypotesise that increasing the number of epochs would increase the network's performance. However, there is a trade-off between performance and the some times expensive process of training. 

        \subsubsection{Choice of cost function}
            We chose MSE as the cost function for the NN. We could have tested various cost functions, e.g. by implementing a penilisation. 

        \subsubsection{Varying Layers, Nodes, and Learning Rate}
            We saw that the optimal network with sigmoid activation function in the hidden layers was our implementation of \network{5}{200} with learning rate 0.001. This was the most complex network we tested, meaning we might get even better results with by further increasing the complexity. However, a very complex model could also lead to overfitting. This network had 161 601 parameters (160 600 weights and 1001 biases). The data we sent in had two input variables, and we sent in 450 datapoints in the training process. In conclusion the model is already rather complex. 
            
            We saw that our and the SciKit-Learn-model had similar performances, implying our implementation is consistent with the norm.

            \comment{exploding gradients!}

        \subsubsection{Activation Functions}
            It is interesting to note that the shape of a predicted graph in some sense inherits the shape of the activation function. The reason the network with the identity activation is linear, is that no non-linearity is ever introduced.
            
            \comment{More occurence of non-converging networks and exploding gradients when employing ReLU. }

            \comment{mention the universal approximation theorem}

        \subsubsection{Regularisation}
            When looking into networks with regularisation, we first used the optimal network from the previous explorations ($\eta = 0.01$) and added various penalisations to it. This resulted in a graph-plot with $\lambda= 10^{-4.1}$ yielding the lowest MSE-value. Subsequently we checked if a network with a different learning rate would perform even better, given some penalisation. We found that this was not the case. The $\eta = 0.01, \lambda = 10^{-4} \text{or} \ \lambda = 10^{-4.1}$ still resulted in the best score. 

            Could have implemented on the gradients of the biases as well as the weights. \comment{Don't know what would've happened though, hehe} If we had employed a stopping criteria in our network, we also would have added it to the cost function. \comment{The last sentence have been mentioned in the method part}

        \subsubsection{Nicaraguan terrain}
            When comparing with the OLS model, we could see, qualitatively and quantitatively, that the network outperformed the linear regression. One could also argue that the network replicated many of the key elements of the true terrain data.  
            
        \subsubsection{Universial approximation theorem}
            \comment{Write something about the large networks we ended up with.}

    \subsection{Classification, Wisconsin Breast Cancer}
        Both Logistic regression and our Neural network managed to achieve a very good accuracy score calculated on the test set. The entire data set contained 569 unique instances, and by using our train-test split of $\sfrac{3}{4}$, the test set contained 143 points. Looking back at \cref{res:tab:accuracy_score_activation_functions}, we found optimal validation accuracies of $0.933$ for many models, using both different network structures and hyperparameters. This corresponds to a single instance being wrongly classified, which one might consider a highly effective network. However, the Wisconsin Breast Cancer data set is known to be ideal and 'noise free' \citep{easyCancer}. Therefor having a repertoire of good performing models is not groundbreaking, meaning that we have not cured cancer. For further analysis, the usage of lower train-test split ratios would be interesting. Model selection and hyperparameter optimisation could be performed as a function of train set size, which could indicate how little train data would be necessary to achieve good results. 
        
        Every feature from the data set has been included for all fitted models. This might be an over complication of the model, where the decisive factors could be a subset of the features present in the data set. Usage of some feature selection techniques would help to reduce the complexity of the model. In \citep{inproceedings}, principal component analysis (PCA) was performed on the Wisconsin Breast Cancer, where they found that the number of features could be reduced to 15 covering 99.6\% explained variance, by making linear combinations of the original features.

        The accuracy score has consistently been used to evaluate models, but looking at the number of false positives (FP) and false negatives (FN) might also be an interesting metric to use. Distinguishing between the type of misclassification could help to narrow down the possible models, where a FN would be worse than a FP.
    

        \subsubsection{Logistic regression}
        Performed good on this set, sufficiently good.

        \subsubsection{Neural network}

\subsection{Model Selection}
    Model selection is prevalent in all parts of this project, and in many cases, the difference between models has been marginal. To get a more thorough examination of which GD methods, regressors and classifiers work best, it would have been helpful to employ resampling techniques like bootstrap or cross-validation, as in~\citep{Project1}, to get better estimates for the scores we have employed throughout, e.g. MSE, R2 and accuracy.