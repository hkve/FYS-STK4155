\comment{Suggestion: future tense}\\
\subsection{Datasets}
    \comment{Here we write about the datasets we use, as before. Maybe reference project 1 about Franke function?}\\
    \comment{Mention data scaling.}\\

\subsection{Assessing Gradient Descent}
    \comment{Trying out the gradient descent methods and comparing them.}\\
    \comment{I have imported data and parameters at random by trial. We can discuss the choices later}\\
    Our first analysis will be a comparison of the various GD algorithms, as this will make the basis for our neural network optimisation. We will test it on the Franke function with 600 data points, with a train-test split of $\sfrac{3}{4}$, and a noise term added with std $\sigma=0.1$.

    First we will minimise the OLS cost function
    \begin{align}
        \msub{C}{OLS}(\vec{\theta}) = \frac{1}{n}\sum_{i=1}^n \pclosed{X_{i*}\vec{\theta} - y_i}^2,
    \end{align}
    with a polynomial expansion of degree $5$ of the Franke function arguments $x,y$. 

    \comment{Mention initialising SGDs with same random seed for every learning rate.}

\subsection{Classification problems}
    Thus far, we have only aimed at solving regression problems. The machinery regarding neural networks we have introduced here, can with some modification be used to approach \textit{classification} problems. Instead of predicting targets in a continuous domain ($y_i \in \mathbb{R}$), we aim to put the target $y_i$ in a specific class ($y_i \in \mathcal{Q}$), where $\mathcal{Q}$ is the set of possible outcomes. 

    In this work we will consider the famous Wisconsin Breast Cancer data set \cite{Dua:2019}. Tumors are characterized as malignant (M) or benign (B), with attributes such as radius, texture and area computed from digitized images. We wish to classify each tumor as either M or B, based on the tumors various attributes. These categories are mutually exclusive, thus determining M or not M is sufficient (i.e. $\mathcal{Q} = \{ M, \neg M \}$). This leaves us with a binary problem. 

    \subsubsection*{Logistic Regression}
    To be continued ...

    \subsubsection*{Neural Network approach}
    \cite{BCE}
    \begin{align}
        \msub{C}{BCE} = -\frac{1}{n}\sum_{i=1}^{n} y_i \log{p_i} + (1-y_i) \log{(1-p_i)}
    \end{align}