\comment{Suggestion: future tense}\\
\subsection{Datasets}
    \comment{Here we write about the datasets we use, as before. Maybe reference project 1 about Franke function?}\\
    \comment{Mention data scaling.}\\

\subsection{Assessing Gradient Descent}
    \comment{Trying out the gradient descent methods and comparing them.}\\
    \comment{I have imported data and parameters at random by trial. We can discuss the choices later}\\
    Our first analysis will be a comparison of the various GD algorithms, as this will make the basis for our neural network optimisation. We will test it on the Franke function with 600 data points, with a train-test split of $\sfrac{3}{4}$, and a noise term added with std $\sigma=0.1$.

    First we will minimise the OLS cost function
    \begin{align}
        \msub{C}{OLS}(\vec{\theta}) = \frac{1}{n}\sum_{i=1}^n \pclosed{X_{i*}\vec{\theta} - y_i}^2,
    \end{align}
    with a polynomial expansion of degree $5$ of the Franke function arguments $x,y$. 

    