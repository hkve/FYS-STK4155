\comment{Suggestion: past tense NB! THIS IS A CHANGE}\\
\subsection{Datasets}
    \comment{Here we write about the datasets we use, as before. Maybe reference project 1 about Franke function?}\\
    We will use some of the same datasets as in Project 1~\citep{Project1}, namely the Franke function data and the Nicaraguan terrain data.
    \subsubsection{Franke Function}
        The Franke function $F: \mathbb{R}^2 \to \mathbb{R}$ is a two-dimensional scalar function made up of four Gaussian functions.
        \begin{align}
            F(x,y) = f_1(x,y) + f_2(x,y) + f_3(x,y) + f_4(x,y) + \epsilon, \label[eq]{met:eq:Franke_Function}
        \end{align}
        where
        \begin{subequations}
            \begin{align}
                f_1(x,y) &= \frac{3}{4}\exptext{ -\frac{(9x-2)^2}{4} - \frac{(9y-2)^2}{4} }, \\
                f_2(x,y) &= \frac{3}{4}\exptext{ -\frac{(9x+1)^2}{49} - \frac{(9y+1)}{10} }, \\
                f_3(x,y) &= \frac{1}{2}\exptext{ -\frac{(9x-7)^2}{4} - \frac{(9y-3)^2}{4} }, \\
                f_4(x,y) &= -\frac{1}{5}\exptext{ -(9x-4)^2 - (9y-7)^2 },
            \end{align}
        \end{subequations}
        and we have added a noise term $\epsilon \distas \normal{0}{\sigma^2}$ with some standard deviation (std) $\sigma$.

    \subsubsection{Terrain Data}
        We have taken terrain data from the Nicaraguan mountains, sampling height data within the square defined by longitudinal coordinates $[86^\circ\,33'\,20''\,\text{W}, 86^\circ\,28'\,20''\,\text{W}]$ and latitudes $[13^\circ\,26'\,40''\,\text{N}, 13^\circ\,31'\,40''\,\text{N}]$. The data has one arcsecond resolution.

    \subsubsection{Wisconsin Breast Cancer data set}\label[sec]{breast_cancer_dataset}
        For regression problems, the Wisconsin Breast Cancer data set \citep{Dua:2019} was used. Tumour features are computed from digitized images, describing characteristics of the cell nuclei present in the image. In total there are $n = 569$ unique instances, where each instance has the respective tumour marked as either malignant ($M$) or benign ($B$). We aimed to classify the tumour as $M$ or $B$ based on 10 different attributes ranging from tumour radius to texture. Each of these 10 attributes were again split up into three different measurements, named mean, standard error and worst. The latter describes the mean of the three largest measurements. This data set serves a good real-world benchmarking set, containing not too many instances and no missing fields.  

    \subsubsection{Data Preparation}
        All the data we handle, we will scale before using it for training and testing our algorithms. We will use the Standard scaling detailed in~\citep{Project1}. This ensures that features are all of the same order of magnitude, making the mean zero and std 1 for all of them individually. In the regression problems, we will also scale the observation data $\vec{y}$ too, whereas this is not done in the binary classification case.

\subsection{Assessing Gradient Descent}
    Our first analysis was a comparison of the various GD algorithms, as these make the basis for our neural network optimisation. We used the GD algorithms to optimise OLS and Ridge cost functions~\citep{Project1}, training them on the Franke function with 600 data points, with a train-test split of $\sfrac{3}{4}$, and a noise term added with std $\sigma=0.1$. This was done in anticipation of applying our Neural Network on terrain data, which the Franke function imitates.

    \subsubsection{OLS Regression}
        First we focused on the OLS cost function
        \begin{align}
            \msub{C}{OLS}(\vec{\theta}) = \frac{1}{n}\sum_{i=1}^n \pclosed{X_{i*}\vec{\theta} - y_i}^2,
            \label[eq]{met:eq:ols_cost_function}
        \end{align}
        with a polynomial expansion of degree $5$ of the Franke function arguments $x,y$. We plot the MSE resulting from the predictions of the optimised parameters on the validation data as a function of learning rates. We looked at learning rates from close to zero to where `exploding gradients' occurred, which was different between the different algorithms.\footnote{Exploding gradients happen when the learning rate is large enough to overshoot the minima, and then start oscillating wildly away from the minima; blowing up.}

        Our preliminary analysis was a comparison of ordinary GD and stochastic GD, with and without momentum. We used 500 iterations for the ordinary GD, and 500 epochs with a batch size of 200 for the stochastic GD. For the momentum methods we used a hyperparameter value $\gamma=0.8$. Furthermore, we investigated the effect of increasing the number of epochs or batches with plain SGD, using 2000 epochs and a batch size of 50.

        Then we looked at algorithms that tune the learning rate, implementing ordinary and stochastic AdaGrad, again with and without momentum. Then we moved on to application of stochastic RMSprop and Adam, and compared how these do in comparison to AdaGrad with SGD.


    \subsubsection{Ridge Regression}
        Adding an $L_2$-penalisation term to the OLS cost function gives us the Ridge cost function
        \begin{align}
            \msub{C}{ridge}(\vec{\theta}) = \frac{1}{n}\sum_{i=1}^n \pclosed{X_{i*}\vec{\theta} - y_i}^2 + \lambda\norm[2]{\vec{\theta}}^2. 
            \label[eq]{met:eq:ridge_cost_function}
        \end{align}

        We selected the four momentum based algorithms from the OLS analysis to see how they responded to changing of learning rate and $\lambda$-parameter in predicting the validation data. Again we used 500 iterations for the GD method, and 500 epochs with batch size 200 with the SGD methods. Varying $\lambda \in [10^{-8}, 10^1]$, we compared first with a fixed learning rate between GD and SGD with momentum ($\gamma=0.8$). Then we used a tunable learning rate and SGD with AdaGrad with momentum ($\gamma=0.8$) and finally Adam $(\beta_1=0.9, \beta_2=0.99)$.

    \subsubsection{Initialisation}
        GD methods are quite sensitive to the initial parameters from which to descend, so made sure to initialise all the methods from the same point when comparing performance. We always initialised with randomly drawn parameter values from the standard normal distribution with mean zero and std one. Since we always deal with scaled features, we can safely initialise the parameters with a standard normal distribution without fear of the dimension of the parameters being terribly out of sorts. To further combat the sensitivity to initial conditions, we did all the descents from five different starting points, and computed the average MSE scores we got.

\subsection{Creating a Neural Network and Regression Problems}
    We started by creating an FFNN with a flexible number of hidden layers and nodes as well as variable learning rate, multiple possible activation functions, and with an optional regularisation parameter. Subsequently we varied these hyperparameters, then trained the various networks and tested them on the Nicaraguan terrain data from Project 1. We compared the best network's performance with the results from the linear regression methods (OLS and Ridge) employed in that project. Our dataset consisted of 600 datapoints in total, and we used a test-train-split of $\sfrac{3}{4}$, meaning our traning data was 450 datapoints. 
    
    \subsubsection{Initialisation of the Network}
        As in the previous section, we set the number of epochs to 500 and batch size to 200. Since we discovered that SGD exceeded GD and that Adam performed best of the GD methods, these algorithms were the ones we employed in our network.    
        \comment{CARL, look over HERE! GD/SGD, GD method}

        In the neural network we chose to implement MSE as the cost function, $C$, for the output. We started by setting the activation of the hidden layers to sigmoid, and since we wanted to perform regression, we selected the identity function for the final output layer. Initially we set the regularisation parameter to zero. 

        When setting up a neural network, the weights and biases need initial values. Choosing these values can affect how the network trains and evetually performes. From \cref{the:eq:bp_w_b_delta} we see that setting all the parameters to $0$ means the network will not backpropogate initially and all the nodes will yield the same output. \comment{is this too theory to be in method?}. In this project we have initialised the weights with a distribution of mean zero and std $1$. We set the initial biases to $0.01$. Since the mean of the initialised weights is 0, setting the initial biases to a small value like this ensures every $z_i^L$ has a value to backpropogate in the first iteration.  
    
    \subsubsection{Varying Layers, Nodes, and Learning Rate}
        We wanted to explore how our neural networks vary depending on their structure. We therefore varied the number of layers, number of nodes in each layer, and the learning rate while computing the measures for goodness of fit. In this case we implemented MSE. See Project 1 for elaboration on these types of measures \citep{Project1}.
        We chose to only evaluate somewhat symmetrical networks, meaning we used the same number of nodes in all the hidden layers.

        To have a compact way of referring to the various network structures we have created, we now indtroduce a new notation. A network will hereby be referred to as \network{layers}{nodes} with the number of hidden layers as a superscript and number of nodes in the hidden layer as a subscript. E.g. a network with 5 hidden layers and 100 nodes in each of those layers will be referred to as \network{5}{100}.  

        We looked at the 1,3, and 5 number of hidden layers, number of nodes between: 2-200, and learning rates in the range: 0.8-0.005. We created three heatplots; one for each \#layers. 

        We also compared with a model from SciKit-Learn with the same hyperparameters as our networks. In that case we chose a network with 3 hidden layers. 

    \subsubsection{Exploring Activation Functions in Hidden Layers}
        As activation functions can largely affect the performance of the network, we explored various implementations. We started by fitting networks to a 1D-data of the polynomial $x^2$ with noise (normal distribution with scale=0.1) and plotted the results. We used 600 datapoints and train-test-split of $\sfrac{3}{4}$. We did this to see the shapes of the approximations of the different activation functions. 
        
        Next we made similar heatplots as before, again traning and testing on the terrain data, but now employing ReLU and leaky ReLU. 

    \subsubsection{Implementing Regularisation}
        Implementing a penalisation only on the weights (not the biases)

    \subsubsection{Terrain Data an Comparison with OLS}
        Lastly we compared the best model with an OLS-model. 
    
\subsection{Classification Problems}
    For our case using the Wisconsin Breast data set presented in \cref{breast_cancer_dataset}, we aimed to classify either $M$ or $B$. These categories are mutually exclusive, thus determining $M$ or not $M$ is sufficient (i.e. $\mathcal{Q} = \{ M, \neg M \}$). This gave us a binary problem, represented with $y_i \in \{ 1, 0 \}$ where $y_i = 1$ implies $M$ and $y_i = 0$ not $M$.

    To measure the goodness of fit in for our classification problems, we used the \textit{accuracy} score. This metric simply counts the number correctly classified cases, divided by the total number of cases:

    \begin{align}
        A(y_i, \hat{y}_i) = \frac{1}{n}\sum_{i=1}^{n} I(\hat{y}_i = y_i), \\
        I(\hat{y}_i = y) = \begin{cases}
            1\hspace{10px}&\text{if }\hat{y}_i = y \\
            0\hspace{10px}&\text{else}
        \end{cases} \nonumber,
    \end{align}
    where $I$ is the indicator function, and $\hat{y}_i \in \{ 1, 0\}$ is our models classification based on the probability $p(x_i)$ and the threshold $\tau$. We note that if all cases are classified correctly, the sum will evaluate to $n$ and if all are classified wrongly, it will evaluate to 0. Thus, the accuracy is bounded by $0 \leq A(y_i, \hat{y}_i) \leq 1$. This will be calculated using the test set from a $\sfrac{3}{4}$ train-test split for both the neural network and logistic regression.

    \subsubsection{Logistic Regression}
    The optimal parameters for our BCE cost function \cref{the:eq:logreg_cost_function} can be found by the various optimisation schemes introduces in \cref{the:sec:gradient_decent_methods}. Taking the BCE cost function, we add an $L_2$-penalisation to have some constraint on the parameter sizes. This gives the cost function

    \begin{align}
        \msub{C}{CLF}(\vec{\theta}) = \msub{C}{BCE}(\vec{\theta}) +  \lambda\norm[2]{\vec{\theta}}^2
        \label[eq]{met:eq:classification_cost_function}.
    \end{align}
    Initially, we investigated how different optimasation methods from the GD and SGD family performed for a range of different learning rates, using the accuracy as scoring. This was done without any penalisation, that is $\lambda = 0$ in \cref{met:eq:classification_cost_function}. GD and mGD ($\gamma = 0.8$) was tried, using 5 and 50 iterations. The SGD based methods Adam SGD $(\beta_1 = 0.9, \beta_2 = 0.99)$ and AdaGrad SGD was tried with 5 epochs, while plain SGD was tried with both 5 and 50 epochs, all using  200 batches. For easier comparisons, all optimastation methods was tried with learning rates  $\eta \in [0.01, 1.0]$.     
    
    Thereafter, to get a better view of how many iterations/epochs was required to achieve a good model, the validation accuracy as a function of iterations/epochs was plotted for different models with a fixed learning rate. Concretely, GD with ($\gamma = 0.8$) and without momentum was tried, in addition to SGD, Adam SGD $(\beta_1 = 0.9, \beta_2 = 0.99)$ and AdaGrad SGD. The learning rate was set to $\eta = 0.05$ for all methods, with iterations/epochs in the range $[0,200]$. SGD based methods all used a batch size of 200. 

    To see how penalisation influenced the model, the validation score was calculated for a range of different penalisation parameters $\lambda$ and learning rates $\eta$. We used five logarithmically spaced penalisation parameters $\lambda \in [10^{-6}, 1]$ and five linearly spaced learning rates $\eta \in [0.001, 0.5]$. Lastly, we used \verb|scikit-learn|'s implementation of logistic regression to compare with the results obtained using our implementation.

    \subsubsection{Neural Network Approach}
    To use our neural network for classification problems, we must apply some restrictions to our neural network. As discussed, since we classify two mutually exclusive outcomes, a single output neuron will be sufficient. Since we wish to interpret this output as the probability for $M$, the output activation function must be bounded by $f(x) \in [0,1]$. Look at our activation function repertoire from \cref{app:activation_functions}, the only applicable functions (without any modifications) is the Sigmoid function. Assuming that the targets we wish to predict are drawn from a normal distribution does not resonate with what we know about classification problems. Therefor, using the OLS/Ridge cost function is not feasible. For this binary problem, a cost function derived form a Bernoulli distribution would be more feasible. The BCE cost function \cref{met:eq:classification_cost_function} is a better choice in this case.     
    
    Despite these restrictions, there are a plethora of components to tweak. Initially, we began investigating how different hidden activation functions performed as a function of learning rate. Specifically, tried Sigmoid, Hyperbolic tangent, ReLU and Leaky ReLU from \cref{app:activation_functions}. This analysis was repeated for four different network structures, \network{1}{5}, \network{1}{10}, \network{2}{5} and \network{3}{5}. Optimisation was done using the overall best GD/SGD method found from our initial tests.  

    Taking complexity and performance into account, we chose two of these structures with two different activation functions and performed an analysis using the $L_2$ penalisation. By varying both $\lambda$ and $\eta$ as hyperparameters, we saw how different network structures behaved with constraints on weight sizes. Lastly, by choosing a specific structure, a comparison with \verb|scikit-learn| was performed.
    % As a loss function suitable for classification problems, the Binary Cross Entropy (BCE) \citep{BCE} was used:
    % \begin{align}
    %     \msub{C}{BCE} = -\frac{1}{n}\sum_{i=1}^{n} y_i \log{\hat{p}_i} + (1-y_i) \log{(1-\hat{p}_i)},
    %     \label{eq:BCE_loss_function}
    % \end{align} 
    % where $\hat{p}_i$ is the predicted probability of $y_i = 1$ given $x_i$.  