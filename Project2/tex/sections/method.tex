\comment{Suggestion: past tense NB! THIS IS A CHANGE}\\
\subsection{Datasets}
    \comment{Here we write about the datasets we use, as before. Maybe reference project 1 about Franke function?}\\
    We will use some of the same datasets as in Project 1~\cite{Project1}, namely the Franke function data and the Nicaraguan terrain data.
    \subsubsection{Franke Function}
        The Franke function $F: \mathbb{R}^2 \to \mathbb{R}$ is a two-dimensional scalar function made up of four Gaussian functions.
        \begin{align}
            F(x,y) = f_1(x,y) + f_2(x,y) + f_3(x,y) + f_4(x,y) + \epsilon, \label[eq]{met:eq:Franke_Function}
        \end{align}
        where
        \begin{subequations}
            \begin{align}
                f_1(x,y) &= \frac{3}{4}\exptext{ -\frac{(9x-2)^2}{4} - \frac{(9y-2)^2}{4} }, \\
                f_2(x,y) &= \frac{3}{4}\exptext{ -\frac{(9x+1)^2}{49} - \frac{(9y+1)}{10} }, \\
                f_3(x,y) &= \frac{1}{2}\exptext{ -\frac{(9x-7)^2}{4} - \frac{(9y-3)^2}{4} }, \\
                f_4(x,y) &= -\frac{1}{5}\exptext{ -(9x-4)^2 - (9y-7)^2 },
            \end{align}
        \end{subequations}
        and we have added a noise term $\epsilon \distas \normal{0}{\sigma^2}$ with some standard deviation (std) $\sigma$.

    \subsubsection{Terrain Data}
        We have taken terrain data from the Nicaraguan mountains, sampling height data within the square defined by longitudinal coordinates $[86^\circ\,33'\,20''\,\text{W}, 86^\circ\,28'\,20''\,\text{W}]$ and latitudes $[13^\circ\,26'\,40''\,\text{N}, 13^\circ\,31'\,40''\,\text{N}]$. The data has one arcsecond resolution.

    \subsubsection{Wisconsin Breast Cancer data set}\label[sec]{breast_cancer_dataset}
        For regression problems, the Wisconsin Breast Cancer data set \cite{Dua:2019} was used. Tumour features are computed from digitized images, describing characteristics of the cell nuclei present in the image. In total there are $n = 569$ unique instances, where each instance has the respective tumour marked as either malignant ($M$) or benign ($B$). We aimed to classify the tumour as $M$ or $B$ based on 10 different attributes ranging from tumour radius to texture. Each of these 10 attributes were again split up into three different measurements, named mean, standard error and worst. The latter describes the mean of the three largest measurements. This data set serves a good real-world benchmarking set, containing not too many instances and no missing fields.  

    \subsubsection{Data Preparation}
        All the data we handle, we will scale before using it for training and testing our algorithms. We will use the Standard scaling detailed in~\cite{Project1}. This ensures that features are all of the same order of magnitude, making the mean zero and std 1 for all of them individually. In the regression problems, we will also scale the observation data $\vec{y}$ too, whereas this is not done in the binary classification case.

\subsection{Assessing Gradient Descent}
    Our first analysis was a comparison of the various GD algorithms, as these make the basis for our neural network optimisation. We used the GD algorithms to optimise OLS and Ridge cost functions~\cite{Project1}, training them on the Franke function with 600 data points, with a train-test split of $\sfrac{3}{4}$, and a noise term added with std $\sigma=0.1$. This was done in anticipation of applying our Neural Network on terrain data, which the Franke function imitates.

    \subsubsection{OLS Regression}
        First we focused on the OLS cost function
        \begin{align}
            \msub{C}{OLS}(\vec{\theta}) = \frac{1}{n}\sum_{i=1}^n \pclosed{X_{i*}\vec{\theta} - y_i}^2,
            \label[eq]{met:eq:ols_cost_function}
        \end{align}
        with a polynomial expansion of degree $5$ of the Franke function arguments $x,y$. We plot the MSE resulting from the predictions of the optimised parameters on the validation data as a function of learning rates. We looked at learning rates from close to zero to where `exploding gradients' occurred, which was different between the different algorithms.\footnote{Exploding gradients happen when the learning rate is large enough to overshoot the minima, and then start oscillating wildly away from the minima; blowing up.}

        Our preliminary analysis was a comparison of ordinary GD and stochastic GD, with and without momentum. We used 500 iterations for the ordinary GD, and 500 epochs with a batch size of 200 for the stochastic GD. For the momentum methods we used a hyperparameter value $\gamma=0.8$. Furthermore, we investigated the effect of increasing the number of epochs or batches with plain SGD, using 2000 epochs and a batch size of 50.

        Then we looked at algorithms that tune the learning rate, implementing ordinary and stochastic AdaGrad, again with and without momentum. Then we moved on to application of stochastic RMSprop and Adam, and compared how these do in comparison to AdaGrad with SGD.


    \subsubsection{Ridge Regression}
        Adding an $L_2$-penalisation term to the OLS cost function gives us the Ridge cost function
        \begin{align}
            \msub{C}{ridge}(\vec{\theta}) = \frac{1}{n}\sum_{i=1}^n \pclosed{X_{i*}\vec{\theta} - y_i}^2 + \lambda\norm[2]{\vec{\theta}}^2. 
            \label[eq]{met:eq:ridge_cost_function}
        \end{align}

        We selected the four momentum based algorithms from the OLS analysis to see how they responded to changing of learning rate and $\lambda$-parameter in predicting the validation data. Again we used 500 iterations for the GD method, and 500 epochs with batch size 200 with the SGD methods. Varying $\lambda \in [10^{-8}, 10^1]$, we compared first with a fixed learning rate between GD and SGD with momentum ($\gamma=0.8$). Then we used a tunable learning rate and SGD with AdaGrad with momentum ($\gamma=0.8$) and finally Adam $(\beta_1=0.9, \beta_2=0.99)$.

    \subsubsection{Initialisation}
        GD methods are quite sensitive to the initial parameters from which to descend, so made sure to initialise all the methods from the same point when comparing performance. We always initialised with randomly drawn parameter values from the standard normal distribution with mean zero and std one. Since we always deal with scaled features, we can safely initialise the parameters with a standard normal distribution without fear of the dimension of the parameters being terribly out of sorts. To further combat the sensitivity to initial conditions, we did all the descents from five different starting points, and computed the average MSE scores we got.

\subsection{Creating a Neural Network and Regression Problems}
    In this part of our analysis we will create an FFNN with a flexible number of hidden layers and nodes. Subsequently we will train and test this network on the Nicaraguan terrain data from Project 1 and compare its performance with the results from the linear regression methods (OLS and Ridge) employed in that project. 

    \comment{Discuss the choice of cost function in the NN}

    We start by implementing the sigmoid activation function in the hidden layers. As 

    
\subsection{Classification Problems}
    For our case using the Wisconsin Breast data set presented in \cref{breast_cancer_dataset}, we aimed to classify either $M$ or $B$. These categories are mutually exclusive, thus determining $M$ or not $M$ is sufficient (i.e. $\mathcal{Q} = \{ M, \neg M \}$). This gave us a binary problem, represented with $y_i \in \{ 1, 0 \}$ where $y_i = 1$ implies $M$ and $y_i = 0$ not $M$.

    To measure the goodness of fit in for our classification problems, we used the \textit{accuracy} score. This metric simply counts the number correctly classified cases, divided by the total number of cases:

    \begin{align}
        A(y_i, \hat{y}_i) = \frac{1}{n}\sum_{i=1}^{n} I(\hat{y}_i = y_i), \\
        I(\hat{y}_i = y) = \begin{cases}
            1\hspace{10px}&\text{if }\hat{y}_i = y \\
            0\hspace{10px}&\text{else}
        \end{cases} \nonumber,
    \end{align}
    where $I$ is the indicator function, and $\hat{y}_i \in \{ 1, 0\}$ is our models classification based on the probability $p(x_i)$ and the threshold $\tau$. We note that if all cases are classified correctly, the sum will evaluate to $n$ and if all are classified wrongly, it will evaluate to 0. Thus, the accuracy is bounded by $0 \leq A(y_i, \hat{y}_i) \leq 1$. This will be calculated using the test set from a $\sfrac{3}{4}$ train-test split for both the neural network and logistic regression.

    \subsubsection{Logistic Regression}
    The optimal parameters for our BCE cost function \cref{the:eq:logreg_cost_function} can be found by the various optimisation schemes introduces in \cref{the:sec:gradient_decent_methods}. Taking the BCE cost function, we add an $L_2$-penalisation to have some constraint on the parameter sizes. This gives the cost function

    \begin{align}
        \msub{C}{CLF}(\vec{\theta}) = \msub{C}{BCE}(\vec{\theta}) +  \lambda\norm[2]{\vec{\theta}}^2
        \label[eq]{met:eq:classification_cost_function}.
    \end{align}
    Initially, we will investigate how different methods from the GD and SGD family perform for a range of different learning rates. This will be done without any penalisation, that is $\lambda = 0$ in \cref{met:eq:classification_cost_function}. FILL IN SPECIFICS OF WHAT METHODS AND PARAMTERS.

    \subsubsection{Neural Network Approach}
    To use our neural network for classification problems, we must make a restriction on our output layer activation functions. As discussed, since we classify two mutually exclusive outcomes, a single output neuron will be sufficient. Since we wish to interpret this output as the probability for $M$, the output activation function must be bounded by $f(x) \in [0,1]$. Look at our activation function repertoire from \cref{app:activation_functions}, the only applicable functions (without any modifications) is the Sigmoid function. 
    
    Despite these restrictions, there are a plethora of components to tweak. Initially, we will begin by investigating how different hidden activation functions perform as a function of learning rate. Specifically, we will try Sigmoid, Hyperbolic tangent, ReLU and Leaky ReLU from \cref{app:activation_functions}. This analysis will be repeated for four different network structures, \network{1}{5}, \network{1}{10}, \network{2}{5} and \network{3}{5}  (GRADIENT DECENT METHOD??).

    Taking complexity and performance into account, we will choose two of these structures with two different activation functions and perform an analysis using the $L_2$ penalisation. By varying both $\lambda$ and $\eta$ as hyperparameters, we see how different network structures behave with constraints on weight sizes.  
    % As a loss function suitable for classification problems, the Binary Cross Entropy (BCE) \cite{BCE} was used:
    % \begin{align}
    %     \msub{C}{BCE} = -\frac{1}{n}\sum_{i=1}^{n} y_i \log{\hat{p}_i} + (1-y_i) \log{(1-\hat{p}_i)},
    %     \label{eq:BCE_loss_function}
    % \end{align} 
    % where $\hat{p}_i$ is the predicted probability of $y_i = 1$ given $x_i$.  