\documentclass[twocolumn,english,notitlepage]{article}
\usepackage[margin=2cm]{geometry}

% \setlength{\parindent}{0pt} % no indents

\usepackage{overhead} % I put the overhead in a separate file

\title{Optimising Simple Neural Networks as Regressors and Classifiers}
\author{Anna Hjertvik Aasen, Carl Martin Fevang, HÃ¥kon Kvernmoen}
\date{\today}

\begin{document}

\twocolumn[
    \begin{@twocolumnfalse}
        \maketitle
        \begin{abstract}
            At first we implemented ten GD and SGD algorithms and optimised OLS and ridge linear regression cost functions. Expanding a polynomial design matrix to degree 5 and fitting to the Franke function, we found SGD with Adam to perform the best, with a validation MSE of $0.1660 \pm 0.0018$, compared to the analytical OLS solution with a validation MSE of $0.1377$. Ridge penalisation did not improve the GD methods, but improved the analytical solution to $0.1338$ with $\lambda=0.002$. \comment{Write on about more results\ldots} With the Wisconsin Breast Cancer, classification was performed using logistic regression and small neural networks. A stable logistic regression model was found with an accuracy of 0.983 with a $L_2$ penalisation term $\lambda = 10^{-3.5}$. Despite achieving an accuracy of 0.993 using a one layer neural network, we argue that logistic regression is sufficient for reliable classification. 
        \end{abstract}
    \end{@twocolumnfalse}
]

\tableofcontents

\section{Introduction}
\input{sections/introduction}

\section{Theory}
\input{sections/theory}

\section{Method}
\input{sections/method}

\section{Results}
\input{sections/results}

\section{Discussion}
\input{sections/discussion}

\section{Concluding remarks}
\input{sections/conclusion}

% \printbibliography
\bibliography{sections/references}

% for numbering appendix equations more appropriately
\numberwithin{equation}{section}
\renewcommand{\theequation}{\thesection.\arabic{equation}}
\newpage
\begin{appendices}
    \input{sections/appendix} 
\end{appendices}


\end{document}

